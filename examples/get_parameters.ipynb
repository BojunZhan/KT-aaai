{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pykt.models import evaluate,evaluate_question,load_model\n",
    "from pykt.datasets import init_test_datasets\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']=':4096:2'\n",
    "\n",
    "with open(\"../configs/wandb.json\") as fin:\n",
    "    wandb_config = json.load(fin)\n",
    "\n",
    "def main(params):\n",
    "    if params['use_wandb'] ==1:\n",
    "        import wandb\n",
    "        os.environ['WANDB_API_KEY'] = wandb_config[\"api_key\"]\n",
    "        wandb.init(project=\"wandb_predict\")\n",
    "\n",
    "    save_dir, batch_size, fusion_type = params[\"save_dir\"], params[\"bz\"], params[\"fusion_type\"].split(\",\")\n",
    "    win200 = params[\"win200\"]\n",
    "\n",
    "    with open(os.path.join(save_dir, \"config.json\")) as fin:\n",
    "        config = json.load(fin)\n",
    "        model_config = copy.deepcopy(config[\"model_config\"])\n",
    "        for remove_item in ['use_wandb','learning_rate','add_uuid','l2']:\n",
    "            if remove_item in model_config:\n",
    "                del model_config[remove_item]    \n",
    "        trained_params = config[\"params\"]\n",
    "        model_name, dataset_name, emb_type, fold = trained_params[\"model_name\"], trained_params[\"dataset_name\"], trained_params[\"emb_type\"], trained_params[\"fold\"]\n",
    "        if model_name in [\"saint\", \"sakt\", \"cdkt\"]:\n",
    "            train_config = config[\"train_config\"]\n",
    "            seq_len = train_config[\"seq_len\"]\n",
    "            model_config[\"seq_len\"] = seq_len   \n",
    "        if model_name in [\"stosakt\"]:\n",
    "            train_args = argparse.ArgumentParser()\n",
    "            args_dict = vars(train_args)\n",
    "            args_dict.update(config[\"train_args\"])\n",
    "            # print(f\"train_args:{train_args.hidden_size}\")\n",
    "\n",
    "    with open(\"../configs/data_config.json\") as fin:\n",
    "        curconfig = copy.deepcopy(json.load(fin))\n",
    "        if model_name in [\"gpt4kt\"]:\n",
    "            dataset_name = params[\"dataset_name\"]\n",
    "        data_config = curconfig[dataset_name]\n",
    "        data_config[\"dataset_name\"] = dataset_name\n",
    "        if model_name in [\"dkt_forget\", \"bakt_time\"] or emb_type.find(\"time\") != -1:\n",
    "            data_config[\"num_rgap\"] = config[\"data_config\"][\"num_rgap\"]\n",
    "            data_config[\"num_sgap\"] = config[\"data_config\"][\"num_sgap\"]\n",
    "            data_config[\"num_pcount\"] = config[\"data_config\"][\"num_pcount\"]\n",
    "        elif model_name in [\"lpkt\"]:\n",
    "            print(\"running  prediction\")\n",
    "            data_config[\"num_at\"] = config[\"data_config\"][\"num_at\"]\n",
    "            data_config[\"num_it\"] = config[\"data_config\"][\"num_it\"] \n",
    "        elif model_name in [\"gpt4kt\"]:\n",
    "            data_config[\"num_q\"] = config[\"data_config\"][\"num_q\"]\n",
    "            data_config[\"num_c\"] = config[\"data_config\"][\"num_c\"] \n",
    "            \n",
    "    # test_loader, test_window_loader, test_question_loader, test_question_window_loader = init_test_datasets(data_config, model_name, batch_size,fold,win200)\n",
    "\n",
    "    print(f\"Start predicting model: {model_name}, embtype: {emb_type}, save_dir: {save_dir}, dataset_name: {dataset_name}\")\n",
    "    print(f\"model_config: {model_config}\")\n",
    "    print(f\"data_config: {data_config}\")\n",
    "\n",
    "    if model_name in [\"stosakt\"]:\n",
    "        model = load_model(model_name, model_config, data_config, emb_type, save_dir, train_args)\n",
    "        print(f\"model_parameter:{sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())}\")\n",
    "    else:\n",
    "        model = load_model(model_name, model_config, data_config, emb_type, save_dir)\n",
    "        print(f\"model_parameter:{sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "'bz': 512,\n",
    "\"fusion_type\":'late_fusion',\n",
    "'use_wandb':0,\n",
    "'dataset_name':'nips_task34',\n",
    "'win200':True,\n",
    "'save_dir':\"/mnt/cfs/project2/full_result_pykt/best_model_path/ednet/sparsekt-topk/sparsekt_tiaocan_ednet_3407_0_0.3_64_256_2_4_0.5_0.5_0.5_50_256_256_8_4_0.0001_0.9_4_1_1_1_98714f5d-3e31-4baf-b9f7-28ce256b7b4f\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting model: sparsekt, embtype: qid_sparseattn, save_dir: /mnt/cfs/project2/full_result_pykt/best_model_path/ednet/sparsekt-topk/sparsekt_tiaocan_ednet_3407_0_0.3_64_256_2_4_0.5_0.5_0.5_50_256_256_8_4_0.0001_0.9_4_1_1_1_98714f5d-3e31-4baf-b9f7-28ce256b7b4f, dataset_name: ednet\n",
      "model_config: {'dropout': 0.3, 'final_fc_dim': 64, 'final_fc_dim2': 256, 'num_layers': 2, 'nheads': 4, 'loss1': 0.5, 'loss2': 0.5, 'loss3': 0.5, 'start': 50, 'd_model': 256, 'd_ff': 256, 'num_attn_heads': 8, 'n_blocks': 4, 'sparse_ratio': 0.9, 'k_index': 4, 'stride': 1}\n",
      "data_config: {'dpath': '../data/ednet', 'num_q': 11901, 'num_c': 188, 'input_type': ['questions', 'concepts'], 'max_concepts': 7, 'min_seq_len': 3, 'maxlen': 200, 'emb_path': '', 'train_valid_original_file': 'train_valid.csv', 'train_valid_file': 'train_valid_sequences.csv', 'folds': [0, 1, 2, 3, 4], 'test_original_file': 'test.csv', 'test_file': 'test_sequences.csv', 'test_window_file': 'test_window_sequences.csv', 'test_question_file': 'test_question_sequences.csv', 'test_question_window_file': 'test_question_window_sequences.csv', 'train_valid_original_file_quelevel': 'train_valid_quelevel.csv', 'train_valid_file_quelevel': 'train_valid_sequences_quelevel.csv', 'test_file_quelevel': 'test_sequences_quelevel.csv', 'test_window_file_quelevel': 'test_window_sequences_quelevel.csv', 'test_original_file_quelevel': 'test_quelevel.csv', 'test_window_file_quelevel_pretrain_w200': 'test_window_sequences_quelevel_pretrain_200.csv', 'test_window_file_quelevel_pretrain': 'test_window_sequences_quelevel_pretrain.csv', 'dataset_name': 'ednet'}\n",
      "model_name: sparsekt, emb_type: qid_sparseattn\n",
      "model_parameter:4610113\n"
     ]
    }
   ],
   "source": [
    "main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
